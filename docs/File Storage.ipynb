{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Saving different files on different formats\n",
    "\n",
    "## ChatGPT advice\n",
    "### Feather vs. Pickle\n",
    "Feather and Pickle are two different file formats used to store data in Python. They have different design goals and use cases.  \n",
    "Feather is a lightweight data interchange format that was developed by Wes McKinney. It is optimized for fast storage and retrieval of data frames, especially when working with data in R or Python. Feather uses the Apache Arrow columnar format and provides an interface for reading and writing data frames efficiently and with low overhead. Feather files are binary and thus not human-readable.  \n",
    "Pickle, on the other hand, is a more general-purpose serialization format in Python. It allows you to save any Python object, including lists, dictionaries, functions, and custom classes, to a file and load it back later. Pickle uses a binary format and is more flexible than Feather, but it can also be slower and more memory-intensive, especially when storing complex data structures. Pickle files are also not human-readable.  \n",
    "In general, if you are working with data frames and need fast storage and retrieval, Feather is a good choice. If you need to save more complex data structures or if you need to store data that can be read and manipulated by other Python programs, Pickle may be a better option.  \n",
    "It's worth noting that both formats have security vulnerabilities when loading data from untrusted sources, so it's recommended to only load data from trusted sources or to validate the data before loading it.\n",
    "\n",
    "### Ading CSV to the mix\n",
    "Comma-Separated Values (CSV) is a simple text-based file format that stores tabular data. Unlike Feather and Pickle, CSV files are human-readable and can be opened and edited in any text editor or spreadsheet software.  \n",
    "CSV is a widely supported format and is compatible with many programming languages, including Python. It can be used to store and exchange data between applications and platforms. However, CSV files can be large and slow to read and write, especially when working with large datasets. In addition, CSV files do not support complex data structures like nested lists or dictionaries.  \n",
    "In general, if you need to exchange data with other applications or platforms, or if you need to store data in a format that is easily accessible and editable by non-technical users, CSV is a good choice. If you need to store and work with large datasets, or if you need to save more complex data structures, you may want to consider using a more efficient format like Feather or Pickle.  \n",
    "In summary, CSV is a good choice for simple, human-readable data storage and exchange, while Feather and Pickle are better suited for fast and efficient storage and retrieval of complex data structures in Python.\n",
    "\n",
    "### And parquet\n",
    "Parquet is a columnar storage format that is optimized for big data processing and storage. Like Apache Arrow, Parquet provides a common data format for in-memory data processing and is designed to be highly efficient for large datasets.  \n",
    "pyarrow provides support for reading and writing Parquet files, making it a good choice for storing and exchanging data between applications and platforms."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Some tests now\n",
    "curtesy to Rob Mulla: \n",
    "- https://www.youtube.com/watch?v=u4rsA5ZiTls\n",
    "- https://gist.github.com/RobMulla/738491f7bf7cfe79168c7e55c622efa5\n",
    "\n",
    "\n",
    "For each format I will create a large pandas dataframe and I will measure the speeds for read/write and space efficiency.\n",
    "\n",
    "> Note: In order for feather format to work you need installed also __pyarrow__."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading and writing CSV\n",
      "CPU times: total: 8.53 s\n",
      "Wall time: 11.8 s\n",
      "CPU times: total: 1.52 s\n",
      "Wall time: 1.92 s\n",
      "Reading and writing Pickle\n",
      "CPU times: total: 31.2 ms\n",
      "Wall time: 36 ms\n",
      "CPU times: total: 15.6 ms\n",
      "Wall time: 37 ms\n",
      "Reading and writing Feather\n",
      "CPU times: total: 93.8 ms\n",
      "Wall time: 81 ms\n",
      "CPU times: total: 93.8 ms\n",
      "Wall time: 74 ms\n",
      "Reading and writing Parquet\n",
      "CPU times: total: 906 ms\n",
      "Wall time: 1.45 s\n",
      "CPU times: total: 734 ms\n",
      "Wall time: 676 ms\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "SIZE_IN_RECS = 2_000_000\n",
    "def get_dataset(size):\n",
    "    # Create Fake Dataset\n",
    "    df = pd.DataFrame()\n",
    "    df['size'] = np.random.choice(['big','medium','small'], size)\n",
    "    df['age'] = np.random.randint(1, 50, size)\n",
    "    df['team'] = np.random.choice(['red','blue','yellow','green'], size)\n",
    "    df['win'] = np.random.choice(['yes','no'], size)\n",
    "    dates = pd.date_range('2020-01-01', '2022-12-31')\n",
    "    df['date'] = np.random.choice(dates, size)\n",
    "    df['prob'] = np.random.uniform(0, 1, size)\n",
    "    return df\n",
    "\n",
    "def set_dtypes(df):\n",
    "    df['size'] = df['size'].astype('category')\n",
    "    df['team'] = df['team'].astype('category')\n",
    "    df['age'] = df['age'].astype('int16')\n",
    "    df['win'] = df['win'].map({'yes':True, 'no': False})\n",
    "    df['prob'] = df['prob'].astype('float32')\n",
    "    return df\n",
    "print('Reading and writing CSV')\n",
    "df = get_dataset(SIZE_IN_RECS)\n",
    "df = set_dtypes(df)\n",
    "%time df.to_csv('test.csv')\n",
    "%time df_csv = pd.read_csv('test.csv')\n",
    "\n",
    "print('Reading and writing Pickle')\n",
    "df = get_dataset(SIZE_IN_RECS)\n",
    "df = set_dtypes(df)\n",
    "%time df.to_pickle('test.pickle')\n",
    "%time df_pickle = pd.read_pickle('test.pickle')\n",
    "\n",
    "print('Reading and writing Feather')\n",
    "df = get_dataset(SIZE_IN_RECS)\n",
    "df = set_dtypes(df)\n",
    "%time df.to_feather('test.feather')\n",
    "%time df_feather = pd.read_feather('test.feather')\n",
    "\n",
    "print('Reading and writing Parquet')\n",
    "df = get_dataset(5_000_000)\n",
    "df = set_dtypes(df)\n",
    "%time df.to_parquet('test.parquet')\n",
    "%time df_parquet = pd.read_parquet('test.parquet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Volume in drive E is Radical Labs\n",
      " Volume Serial Number is 442C-98BD\n",
      "\n",
      " Directory of e:\\Work\\A5\\production_architecture\\luigi\\docs\n",
      "\n",
      "09/02/2023  11:09    <DIR>          .\n",
      "09/02/2023  11:09    <DIR>          ..\n",
      "09/02/2023  10:38               256 File Storage.ipynb\n",
      "09/02/2023  11:08        99,112,683 test.csv\n",
      "09/02/2023  11:09        20,511,210 test.feather\n",
      "09/02/2023  11:09        34,428,771 test.parquet\n",
      "09/02/2023  11:09        34,001,704 test.pickle\n",
      "               5 File(s)    188,054,624 bytes\n",
      "               2 Dir(s)  267,657,674,752 bytes free\n"
     ]
    }
   ],
   "source": [
    "! dir"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Conclusions\n",
    "- For now I will go with __pickle__ because is the fastest, and, also because is not limited to pandas.\n",
    "- __Parquet__ has no advantages against other formats so is the first to be let go.\n",
    "- __Feather__ offer a good storage on disk, this is important but in our environment must see if pyarrow comes preinstalled.\n",
    "- __CSV__ is good alternative for small files that need to be seen by humans and also there is a performant visualization tool in jupyter for it."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7 (default, Sep 16 2021, 16:59:28) [MSC v.1916 64 bit (AMD64)]"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "77ad8c104b73c8f9df0b42becad597bc5627d294a4305e3f24afed84351a1160"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
